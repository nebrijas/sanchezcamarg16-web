{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc95d2c7",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ffdf1",
   "metadata": {},
   "source": [
    "Esta la actividad dirigida 3 que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación  con Pyton donde realizamos *web scraping*\n",
    "A continuación pongo el código fuente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f90ae",
   "metadata": {},
   "source": [
    "### Programación Literaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f15416",
   "metadata": {},
   "source": [
    "## Módulos y librerías del sistema\n",
    "Vamos a importar librerias y módulos del sistema:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4cee0",
   "metadata": {},
   "source": [
    "## Módulos internos del sistema:\n",
    "- [time](https://docs.python.org/3/library/time.html): Proporciona varias funciones con el tiempo\n",
    "- [csv](https://docs.python.org/3/library/csv.html): Este módulo sirve para la escritura y lectura de archivos, separados por coma.\n",
    "- [re](https://docs.python.org/es/3/library/re.html): Proporciona operaciones de coincidencia de expresiones regulares similares a Perl.\n",
    "- [os](https://docs.python.org/3/library/os.html): Este módulo proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9df837",
   "metadata": {},
   "source": [
    "## Librerías internas\n",
    "- [Requests](https://requests.readthedocs.io/en/latest/):Facilita los trabajos con peticiones de HTTP.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/): Facilita extraer información de páginas web.\n",
    "- [pandas](https://pandas.pydata.org/): Es una herramienta de manipulación y análisis de datos de código abierto rápida.\n",
    "- [termcolor](https://pypi.org/project/termcolor/): Permite exprimir texto coloreado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46ead4",
   "metadata": {},
   "source": [
    "## Instalamos librerías\n",
    "Las librerias que vienen con Phyton no hay que instalarlas, pero otras si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a82e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\agroc\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: termcolor in c:\\users\\agroc\\appdata\\roaming\\python\\python39\\site-packages (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddf553",
   "metadata": {},
   "source": [
    "## Importamos librerías\n",
    "A parte de las funciones básicas, Phython cuenta con una serie de bibliotecas que ya fueron programadas y se pueden importar para ser utilizadas a mi programa principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127bca3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0deb0",
   "metadata": {},
   "source": [
    "## Objetivos variables\n",
    "Vamos a crear una serie de objetos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721629e2",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "Creamos un objeto vacío denominado resultados en que se añadierán los resultados de los **scrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc62006",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04b1fe",
   "metadata": {},
   "source": [
    "Escogí algunos códigos. En este primera función se realizó una petición de sección de Internacional del Diario El País. Se realizó la petición con el método req = requests.get(+URL), la condicional if (req.status_code != 200) señala que si el estatus code no es 200 no se puede leer la página. Posteriormente se analiza el HTML de la página en BeautifulSoup(req.text, 'html.parser'), ya por último tira los HTML de la página y los imprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3c6d0",
   "metadata": {},
   "source": [
    "Ucrania teme una situación “aterradora” en Donbás ante los avances del ejército de Rusia\n",
    "Rumania prepara una ley que recupera las prácticas de delación de la época comunista \n",
    "La UE da la mayor muestra de apoyo a Ucrania al avalar su candidatura al bloque comunitario \n",
    "Exiliados rusos\n",
    "Cambio en Colombia\n",
    "Las elecciones reparlamentarizan Francia\n",
    "Emmanuel Macron pierde las legislativas en Francia \n",
    "¿La democracia es una sola mujer trabajadora en la Asamblea de Francia?\n",
    "El Supremo de Estados Unidos consagra el derecho a llevar armas en público\n",
    "Vídeo | Rusia vuelve a mirar a Járkov: análisis de la ofensiva de Putin en el este de Ucrania\n",
    "Turquía desarticula una red iraní que planeaba atentar contra israelíes en Estambul\n",
    "El multimillonario Mikati recibe el encargo de formar Gobierno en Líbano tras años de crisis \n",
    "La candidatura de Ucrania espolea la impaciencia de los Balcanes para ingresar en la UE \n",
    "El ministro de Infraestructuras de Portugal: “Es imperdonable que portugueses y españoles no resolvamos las conexiones ferroviarias”\n",
    "Alemania activa la segunda fase de su plan de emergencia ante la escasez de gas tras los recortes de suministro de Rusia \n",
    "Reformas en justicia, corrupción y oligarcas: la carrera de obstáculos que Ucrania debe recorrer para entrar en la UE \n",
    "Congo recupera de Bélgica lo único que queda de su héroe nacional, Patrice Lumumba: un diente\n",
    "Los talibanes, aislados y sin recursos, piden ayuda internacional para la emergencia del terremoto en Afganistán\n",
    "Más de un millar de muertos en el sudeste de Afganistán por un terremoto de magnitud 6,1\n",
    "La ayuda estatal a los más pobres en Argentina profundiza la disputa por el poder en la Casa Rosada\n",
    "Demócratas y republicanos alcanzan el primer acuerdo en décadas para el control de armas en Estados Unidos\n",
    "La Corte Suprema argentina deja libre el camino para juzgar a Cristina Kirchner en una causa por corrupción\n",
    "La comisión del 6 de enero se centra en las presiones y amenazas de Trump a varios funcionarios  para que no certificaran el triunfo de Biden\n",
    "Ecuador advierte de que “la democracia está en serio riesgo” por las protestas indígenas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8969816",
   "metadata": {},
   "source": [
    "### Parte Código 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0722a7",
   "metadata": {},
   "source": [
    "Este cófigo tiene similiares peticiones a la anterior (requests.get,if (req.status_code != 200 BeautifulSoup(req3.text, 'html.parser'), soup3.findAll) solo que está vez en la sección de opinión. También solicitados en HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7983c4",
   "metadata": {},
   "source": [
    "Nuevo paquete anticrisis...\n",
    "…porque la guerra de Putin es imprevisible\n",
    "El castellano es el fracaso \n",
    "Parole, parole, parole…\n",
    "Vidas gitanas\n",
    "El leninismo reaccionario ha ganado\n",
    "Cierra las piernas, niña\n",
    "El aborto y la guerra fría\n",
    "Si la OTAN no existiera \n",
    "América Latina, ‘quo vadis?’\n",
    "Perder el sentido de la oportunidad\n",
    "Boca abierta\n",
    "Aborto en EE UU: medio siglo hacia atrás\n",
    "El Roto\n",
    "Peridis\n",
    "Flavita Banana\n",
    "Riki Blanco\n",
    "Sciammarella\n",
    "Planeta de Plástico, por Malagón\n",
    "Envía tu carta\n",
    "Réquiem por la sierra de la Culebra  \n",
    "Harta de los negacionistas\n",
    "Desmantelando la sanidad, que es gerundio\n",
    "Opinar sin insultar\n",
    "Contra el conflicto de intereses, transparencia\n",
    "Entre los derechos de Esther López y los de los lectores\n",
    "El defensor del lector contesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77289afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8fce4",
   "metadata": {},
   "source": [
    "Titulares extraídos de la televisión, se realizó la solicitud de requests.get, luego if (req.status_code != 200) que advierte que si el estatus code no es 200 no se puede leer la página y recuerda que no se puede hacer Web Scraping en\"+ URL. El HTML se trata en BeautifulSoup(req16.text, 'html.parser'). Tras obteber los HTML, se imprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ecf531",
   "metadata": {},
   "source": [
    "‘La clave’ de Balbín, gloria y caída del programa que cambió la televisión española\n",
    "Un viaje en el tiempo con Kurt Vonnegut\n",
    "Muere José Luis Balbín, mítico presentador y creador del programa ‘La clave’\n",
    "Maria Ressa, la periodista filipina “no exenta de ser asesinada”\n",
    "‘Supervivientes’ y la incultura general\n",
    "‘Intimidad’ o la denuncia de un delito del siglo XXI \n",
    "‘Intimidad’, protagonizada por Santi Millán\n",
    "El TSJ de Madrid declara nulo el despido del guionista de TVE que escribió el rótulo “Leonor se va de España, como su abuelo”\n",
    "‘Legacy’: un Bosch sin placa y un poco perdido\n",
    "Claves de la nueva Ley General de Comunicación Audiovisual, más allá del enfado de los productores independientes\n",
    "‘Sanditon’ resucita a Jane Austen\n",
    "Ciencia, tecnología y entrevistas en las novedades de la programación de verano en la Cadena SER\n",
    "Los abonados a las plataformas y televisiones de pago superan el número de hogares españoles \n",
    "‘Intimidad’, protagonizada por Santi Millán\n",
    "La historia de Warren Jeffs, un profeta tirano, violador y líder mormón\n",
    "‘Pasapalabra’ busca a su próxima gran estrella\n",
    "HBO ata a Kit Harington para una posible secuela de ‘Juego de tronos’ centrada en Jon Nieve\n",
    "¿Qué ver hoy en TV? Viernes 24 de junio de 2022\n",
    "Nueve capítulos para recordar ‘The Wire’ en su 20º aniversario\n",
    "Harry Palmer: el tercer vértice del mágico triángulo de espías británicos\n",
    "Las series de junio de 2022: ‘The Boys’ en Amazon Prime Video; ‘Peaky Blinders’ en Netflix y otras\n",
    "La fuerza acompaña a ‘Obi-Wan Kenobi’, una serie deslumbrante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3c71a",
   "metadata": {},
   "source": [
    "## Código fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    " \n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    " \n",
    "tags = soup.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    " \n",
    "tags = soup4.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    " \n",
    "tags = soup5.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    " \n",
    "tags = soup6.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    " \n",
    "tags = soup8.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    " \n",
    "tags = soup9.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    " \n",
    "tags = soup10.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    " \n",
    "tags = soup11.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    " \n",
    "tags = soup13.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    " \n",
    "tags = soup14.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    " \n",
    "tags = soup15.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    " \n",
    "tags = soup17.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    " \n",
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
